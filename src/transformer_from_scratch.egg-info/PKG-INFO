Metadata-Version: 2.4
Name: transformer_from_scratch
Version: 0.1.0
Summary: A Transformer model from scratch.
Requires-Python: >=3.12
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.8.0
Requires-Dist: torchaudio>=2.8.0
Requires-Dist: torchvision>=0.23.0
Requires-Dist: pandas
Requires-Dist: matplotlib
Requires-Dist: xformers
Requires-Dist: datasets
Requires-Dist: tokenizers
Requires-Dist: tqdm

# Transformer from Scratch

This repository contains a PyTorch implementation of a Transformer model, built from scratch. The model is trained on the WikiText-103 dataset for language modeling.

## Setup

This project uses `uv` for package management. To get started, follow these steps:

1.  **Install uv**:

    ```bash
    pip install uv
    ```

2.  **Create a virtual environment**:

    ```bash
    uv venv
    ```

3.  **Activate the virtual environment**:

    ```bash
    source .venv/bin/activate
    ```

4.  **Install dependencies**:

    ```bash
    uv pip install -e .
    ```

## Usage

The training scripts can be run as modules. For example, to run the small training script:

```bash
python -m transformer_from_scratch.training_small
```

## Transformer Architecture

The Transformer model is composed of an encoder and a decoder.

### Encoder

The encoder consists of a stack of identical layers. Each layer has two sub-layers:

1.  **Multi-Head Self-Attention:** This layer allows the model to weigh the importance of different words in the input sequence when encoding a specific word.
2.  **Position-wise Feed-Forward Network:** This is a fully connected feed-forward network applied to each position separately and identically.

Each sub-layer is followed by a residual connection and layer normalization.

### Decoder

The decoder is also composed of a stack of identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, each sub-layer is followed by a residual connection and layer normalization. The self-attention sub-layer in the decoder is modified to prevent positions from attending to subsequent positions.

## Training Process

The training process is handled by the `training.py` script. It performs the following steps:

1.  **Data Loading and Preprocessing:** The script loads the WikiText-103 dataset and preprocesses it by tokenizing the text using a Byte-Pair Encoding (BPE) tokenizer.
2.  **Model Initialization:** The Transformer model is initialized with the specified hyperparameters.
3.  **Training Loop:** The model is trained using the Adam optimizer and Cross-Entropy loss. The training loop iterates over the dataset for a specified number of epochs, and for each epoch, it iterates over the training data in batches.
4.  **Evaluation:** After each epoch, the model is evaluated on the validation set to monitor its performance.
5.  **Model Saving:** The best performing model (based on validation loss) is saved to a file.

There are also `training_small.py` and `training_large.py` scripts for training smaller and larger versions of the model, respectively.

## Evaluation

The `profiling.ipynb` Jupyter notebook is used for a detailed evaluation and performance analysis of the trained model. The notebook covers the following aspects:

-   **GPU Memory and Throughput Analysis:** It analyzes the model's GPU memory usage and throughput under different batch sizes. This helps in understanding the trade-offs between batch size, memory consumption, and processing speed.
-   **Context Length Scaling:** The notebook investigates how the model's memory usage scales with the context length, demonstrating the quadratic memory complexity of the self-attention mechanism.
-   **Mixed Precision Profiling:** It compares the performance of the model when using full precision (FP32) versus automatic mixed precision (AMP). This analysis highlights the benefits of AMP in terms of memory reduction and throughput improvement, especially on modern GPUs with Tensor Cores.
-   **Perplexity vs. Context Length:** The notebook evaluates the model's perplexity on the validation set with varying context lengths. This helps in understanding how the model's performance is affected by the amount of context it can consider.


The notebook includes visualizations that plot these metrics, providing a clear picture of the model's performance characteristics.
